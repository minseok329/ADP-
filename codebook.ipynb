{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db69c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 기본 라이브러리 및 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 폰트 설정 (필요시)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 통계분석 라이브러리\n",
    "from scipy import stats\n",
    "from scipy.stats import (\n",
    "    ttest_1samp, ttest_rel, ttest_ind,     # t-검정\n",
    "    chisquare, chi2_contingency,           # 카이제곱 검정\n",
    "    pearsonr, spearmanr, kendalltau,       # 상관분석\n",
    "    shapiro, normaltest,                   # 정규성 검정\n",
    "    levene, bartlett,                      # 등분산성 검정\n",
    "    mannwhitneyu, wilcoxon, ranksums,      # 비모수 검정\n",
    "    binom, f_oneway                        # 이항분포, ANOVA\n",
    ")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# 생존분석\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# 간편 통계\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dcd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 머신러닝 라이브러리\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.feature_selection import SelectKBest, RFE, RFECV, SequentialFeatureSelector\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 머신러닝 모델\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# 부스팅\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# 평가지표\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# 고급도구\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 기본 EDA 및 데이터 탐색\n",
    "\n",
    "# 데이터 기본 정보 확인\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "df.shape\n",
    "df.head()\n",
    "\n",
    "# 결측치 및 고유값 확인\n",
    "df.isnull().sum()\n",
    "df.nunique()\n",
    "\n",
    "# 각 컬럼별 value_counts 확인 (ADP 단골)\n",
    "for col in df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())\n",
    "\n",
    "# 데이터 타입별 분리\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "#그룹별 데이터 분리\n",
    "group_A = df[df['school_name'] == 'A']['score'].dropna()\n",
    "group_B = df[df['school_name'] == 'B']['score'].dropna()\n",
    "\n",
    "\n",
    "# 기본 통계량 확인\n",
    "df[num_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 데이터 시각화\n",
    "\n",
    "# 히스토그램 분포 확인 (ADP 필수)\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(num_cols):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(df[col])\n",
    "        axes[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 각 컬럼별 박스플롯 이상치 확인\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.title(f'{col}의 boxplot')\n",
    "    plt.boxplot(df[col])\n",
    "    plt.show()\n",
    "\n",
    "# 상관관계 히트맵 (pearson은 연속형, spearman은 순서형)\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(numeric_only=True, method='pearson'), \n",
    "           cmap='coolwarm', vmin=-1, vmax=1, annot=True)\n",
    "plt.title('상관관계 히트맵')\n",
    "plt.show()\n",
    "\n",
    "# 종속변수 기준 각 독립변수 차이 확인\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title(f'Category별 {col}의 분포')\n",
    "    sns.boxplot(data=df, x='Category', y=col)\n",
    "    plt.show()\n",
    "    \n",
    "# VIF 계수를 보기 전에 문자형 변수 -> 수치형으로 인코딩 필요\n",
    "df_vif = pd.get_dummies(df, drop_first=True)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "# VIF (다중공선성 확인)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"VIF > 10: 다중공선성 심각, VIF > 5: 주의\")\n",
    "print(vif_data[vif_data['VIF'] > 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 결측치 및 이상치 처리\n",
    "\n",
    "# 결측치 처리 방법들\n",
    "# 방법1: 행 전체 삭제\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# 방법2: 통계치로 대체 (정규분포 확인 후)\n",
    "from scipy.stats import shapiro\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        stat, p = shapiro(df[col].dropna())\n",
    "        print(f'{col}의 Shapiro-Wilk p-value: {p:.4f}, '\n",
    "             f'{\"정규분포 가정 만족\" if p >= 0.05 else \"정규분포 아님\"}')\n",
    "        \n",
    "        # 정규분포면 평균, 아니면 중앙값으로 대체\n",
    "        if p >= 0.05:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# 이상치 처리\n",
    "# 방법1: IQR 방식으로 탐지\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    outlier_idx = df[(df[col] < q1 - 1.5*iqr) | (df[col] > q3 + 1.5*iqr)].index\n",
    "    print(f\"{col} 이상치: {len(outlier_idx)}개 ({len(outlier_idx)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 방법2: Winsorization (상하위 5% 경계값으로 제한)\n",
    "from scipy.stats.mstats import winsorize\n",
    "df_winsorized = df.copy()\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    df_winsorized[col] = winsorize(df[col], limits=[0.05, 0.05])\n",
    "\n",
    "# 방법3: 로그변환 (right-skewed 데이터용)\n",
    "for col in ['income', 'price']:  # 예시 컬럼\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_log'] = np.log1p(df[col])  # log(1+x) 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-2. 시계열 및 고급 전처리\n",
    "\n",
    "# 날짜/시간 데이터 처리\n",
    "df['date'] = pd.to_datetime(df['date'])  # 문자열 → 날짜형 변환\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['dayofweek'] = df['date'].dt.dayofweek  # 0:월요일, 6:일요일\n",
    "df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 날짜 간 차이 계산\n",
    "df['days_since'] = (df['end_date'] - df['start_date']).dt.days\n",
    "df['age_years'] = (pd.Timestamp.now() - df['birth_date']).dt.days // 365\n",
    "\n",
    "# 범주형 변수 빈도 기반 인코딩\n",
    "freq_map = df['category'].value_counts().to_dict()\n",
    "df['category_freq'] = df['category'].map(freq_map)\n",
    "\n",
    "# 희소 범주 통합 (빈도 5 미만을 'Others'로)\n",
    "min_freq = 5\n",
    "rare_categories = df['category'].value_counts()\n",
    "rare_list = rare_categories[rare_categories < min_freq].index.tolist()\n",
    "df['category_clean'] = df['category'].replace(rare_list, 'Others')\n",
    "\n",
    "# 구간화 (Binning)\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 20, 40, 60, 100], \n",
    "                        labels=['청소년', '청년', '중년', '노년'])\n",
    "df['score_quartile'] = pd.qcut(df['score'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# 파생변수 생성\n",
    "df['bmi'] = df['weight'] / (df['height'] / 100) ** 2  # BMI 계산\n",
    "df['income_per_person'] = df['household_income'] / df['family_size']  # 1인당 소득\n",
    "\n",
    "# 로그 스케일링 및 정규화\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')  # Box-Cox보다 안정적\n",
    "df['income_transformed'] = pt.fit_transform(df[['income']])\n",
    "\n",
    "# 이상치 캡핑 (상하위 1% 제한)\n",
    "def cap_outliers(series, lower_percentile=1, upper_percentile=99):\n",
    "    lower_cap = series.quantile(lower_percentile / 100)\n",
    "    upper_cap = series.quantile(upper_percentile / 100)\n",
    "    return series.clip(lower_cap, upper_cap)\n",
    "df['salary_capped'] = cap_outliers(df['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 피처 엔지니어링\n",
    "\n",
    "# 범주형 변수 인코딩\n",
    "# 방법1: Label Encoder (순서가 있는 경우)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['grade_encoded'] = le.fit_transform(df['grade'])\n",
    "\n",
    "# 방법2: One-Hot Encoding (순서가 없는 경우)\n",
    "df_encoded = pd.get_dummies(df, columns=['category'], prefix='category')\n",
    "\n",
    "# 원핫인코딩 방법2: sklearn OneHotEncoder (고급)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_cols = ['성별', '지역', '등급']\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "X_train_enc = encoder.fit_transform(X_train[cat_cols])\n",
    "X_test_enc = encoder.transform(X_test[cat_cols])\n",
    "feature_names = encoder.get_feature_names_out(cat_cols)\n",
    "X_train_enc_df = pd.DataFrame(X_train_enc, columns=feature_names, index=X_train.index)\n",
    "X_train_final = X_train.drop(columns=cat_cols).join(X_train_enc_df)\n",
    "\n",
    "\n",
    "# 방법3: Target Encoding (범주별 타겟 평균)\n",
    "target_means = df.groupby('category')['target'].mean()\n",
    "df['category_target_encoded'] = df['category'].map(target_means)\n",
    "\n",
    "# 수치형 변수 변환\n",
    "# 표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df.select_dtypes(include='number'))\n",
    "\n",
    "# 정규화 (0~1 범위)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax = minmax_scaler.fit_transform(df.select_dtypes(include='number'))\n",
    "\n",
    "# 로버스트 스케일링 (이상치 영향 최소화)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "df_robust = robust_scaler.fit_transform(df.select_dtypes(include='number'))\n",
    "\n",
    "# 새로운 피처 생성\n",
    "# 파생변수 예시\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 20, 30, 40, 50, 100], labels=['teen', 'twenties', 'thirties', 'forties', 'senior'])\n",
    "df['bmi'] = df['weight'] / (df['height'] / 100) ** 2  # BMI 계산 예시\n",
    "df['income_per_family'] = df['income'] / df['family_size']  # 가족 1인당 소득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c58da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 데이터 분할\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 기본 분할 (7:3 비율)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 층화 분할 (클래스 비율 유지)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# 시계열 데이터 분할 (순서 보장)\n",
    "# 방법1: 시간 기준 고정 분할\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[train_size:]\n",
    "\n",
    "# 방법2: 사용자 정의 분할 함수\n",
    "def custom_train_test_split(df, target_col, test_size=0.3, random_state=42):\n",
    "    \"\"\"사용자 정의 학습/테스트 분할 함수\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 카테고리별 분할 (예: 범주형 변수 고려)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for category in df['category'].unique():\n",
    "        cat_indices = df[df['category'] == category].index.tolist()\n",
    "        np.random.shuffle(cat_indices)\n",
    "        \n",
    "        n_test = int(len(cat_indices) * test_size)\n",
    "        test_indices.extend(cat_indices[:n_test])\n",
    "        train_indices.extend(cat_indices[n_test:])\n",
    "    \n",
    "    X_train = df.loc[train_indices].drop(target_col, axis=1)\n",
    "    X_test = df.loc[test_indices].drop(target_col, axis=1)\n",
    "    y_train = df.loc[train_indices][target_col]\n",
    "    y_test = df.loc[test_indices][target_col]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 데이터 분할 정보 확인\n",
    "print(f\"Train set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"Train target distribution:\\n{pd.Series(y_train).value_counts(normalize=True)}\")\n",
    "print(f\"Test target distribution:\\n{pd.Series(y_test).value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 불균형 데이터 처리\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# 클래스 불균형 확인\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "\n",
    "# 방법1: 언더샘플링 (다수 클래스 데이터 줄이기)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "print(\"After undersampling:\", Counter(y_resampled))\n",
    "\n",
    "# 방법2: 오버샘플링 (소수 클래스 데이터 늘리기)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "print(\"After oversampling:\", Counter(y_resampled))\n",
    "\n",
    "# 방법3: SMOTE (합성 데이터 생성)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", Counter(y_resampled))\n",
    "\n",
    "# 방법4: SMOTEENN (SMOTE + 편집된 최근접 이웃)\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTEENN:\", Counter(y_resampled))\n",
    "\n",
    "# 가중치 기반 처리 (클래스별 가중치 자동 계산)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 통계적 검정\n",
    "\n",
    "from scipy.stats import shapiro, normaltest, ttest_1samp, ttest_ind, mannwhitneyu, chi2_contingency, f_oneway, kruskal, levene\n",
    "import pingouin as pg\n",
    "\n",
    "# 정규성 검정\n",
    "# Shapiro-Wilk Test (표본 크기 < 5000)\n",
    "stat, p = shapiro(data)\n",
    "print(f'Shapiro-Wilk p-value: {p:.4f}')\n",
    "print(\"정규분포를 따른다\" if p >= 0.05 else \"정규분포를 따르지 않는다\")\n",
    "\n",
    "# D'Agostino and Pearson's Test (대용량 데이터)\n",
    "stat, p = normaltest(data)\n",
    "print(f\"D'Agostino p-value: {p:.4f}\")\n",
    "\n",
    "# 등분산성 검정  \n",
    "stat, p = levene(group1, group2)\n",
    "print(f\"Levene 검정: 통계량={stat:.4f}, p-value={p:.4f}\")\n",
    "\n",
    "# 평균 비교 검정\n",
    "# 1표본 t-검정 (모집단 평균과 비교)\n",
    "stat, p = ttest_1samp(sample_data, population_mean)\n",
    "print(f'1-sample t-test p-value: {p:.4f}')\n",
    "\n",
    "# 2표본 t-검정 (두 그룹 평균 비교)\n",
    "stat, p = ttest_ind(group1, group2, equal_var=True)  # 등분산 가정\n",
    "stat, p = ttest_ind(group1, group2, equal_var=False)  # 이분산 가정\n",
    "print(f'2-sample t-test p-value: {p:.4f}')\n",
    "\n",
    "# Mann-Whitney U 검정 (비모수적 방법)\n",
    "stat, p = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "print(f'Mann-Whitney U p-value: {p:.4f}')\n",
    "\n",
    "# McNemar 검정 (대응표본, 범주형)\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "ct = pd.crosstab(df['Before'], df['After'])\n",
    "result = mcnemar(ct.values, exact=False, correction=True)\n",
    "\n",
    "# 분산분석 (ANOVA)\n",
    "# 일원분산분석 (3개 이상 그룹)\n",
    "stat, p = f_oneway(group1, group2, group3)\n",
    "print(f'One-way ANOVA p-value: {p:.4f}')\n",
    "\n",
    "# 방법2\n",
    "groups = [df[df['group'] == name]['value'] for name in df['group'].unique()]\n",
    "f_stat, p = f_oneway(*groups)\n",
    "\n",
    "# Kruskal-Wallis 검정 (비모수적 ANOVA)\n",
    "stat, p = kruskal(group1, group2, group3)\n",
    "print(f'Kruskal-Wallis p-value: {p:.4f}')\n",
    "\n",
    "# 카이제곱 검정 (범주형 변수 독립성)\n",
    "contingency_table = pd.crosstab(df['var1'], df['var2'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f'Chi-square p-value: {p:.4f}')\n",
    "\n",
    "\n",
    "# Tukey HSD 사후검정\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "tukey_result = pairwise_tukeyhsd(endog=df['value'], groups=df['group'], alpha=0.05)\n",
    "print(tukey_result)\n",
    "\n",
    "\n",
    "# 이항분포 확률계산\n",
    "from scipy.stats import binom\n",
    "prob = binom.pmf(k=3, n=25, p=0.03)  # 25개 중 3개 불량일 확률\n",
    "\n",
    "# 베이즈 정리\n",
    "#P(A|B) = P(B|A) × P(A) / P(B)\n",
    "prob_a_given_b = (prob_b_given_a * prob_a) / prob_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 차원 축소 (PCA)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 표준화 (PCA 전 필수)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA 수행\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 설명된 분산 비율 확인\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# 주성분별 설명 분산 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Component')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 95% 분산을 설명하는 주성분 개수 찾기\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"95% 분산을 설명하는 주성분 개수: {n_components_95}\")\n",
    "\n",
    "# 최적 주성분 개수로 PCA 재수행\n",
    "pca_final = PCA(n_components=n_components_95)\n",
    "X_pca_reduced = pca_final.fit_transform(X_scaled)\n",
    "print(f\"차원 축소: {X.shape[1]} → {X_pca_reduced.shape[1]}\")\n",
    "\n",
    "# 주성분 기여도 확인 (각 변수가 주성분에 미치는 영향)\n",
    "components_df = pd.DataFrame(\n",
    "    pca_final.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components_95)],\n",
    "    index=X.columns\n",
    ")\n",
    "print(\"주요 변수별 기여도:\")\n",
    "print(components_df.abs().sum(axis=1).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b467d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 기본 머신러닝 모델\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 분류 모델들\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# 회귀 모델들\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'KNN': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# 분류 모델 학습 및 예측\n",
    "classification_results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
    "    classification_results[name] = {'predictions': y_pred, 'probabilities': y_prob}\n",
    "    print(f\"{name} 학습 완료\")\n",
    "\n",
    "# 회귀 모델 학습 및 예측\n",
    "regression_results = {}\n",
    "for name, reg in regressors.items():\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    regression_results[name] = y_pred\n",
    "    print(f\"{name} 학습 완료\")\n",
    "\n",
    "# 피처 중요도 확인 (Random Forest 예시)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 중요한 피처:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. 모델 평가 지표\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "                           mean_squared_error, mean_absolute_error, r2_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 분류 모델 평가\n",
    "def evaluate_classification(y_true, y_pred, y_prob=None, model_name=\"Model\"):\n",
    "    \"\"\"분류 모델 종합 평가 함수\"\"\"\n",
    "    print(f\"=== {model_name} 평가 결과 ===\")\n",
    "    \n",
    "    # 기본 지표들\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"정확도(Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"정밀도(Precision): {precision:.4f}\")\n",
    "    print(f\"재현율(Recall): {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # ROC-AUC (이진 분류일 때)\n",
    "    if len(np.unique(y_true)) == 2 and y_prob is not None:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    # 분류 보고서\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 회귀 모델 평가\n",
    "def evaluate_regression(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"회귀 모델 종합 평가 함수\"\"\"\n",
    "    print(f\"=== {model_name} 평가 결과 ===\")\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    # 실제값 vs 예측값 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{model_name} - Actual vs Predicted')\n",
    "    plt.show()\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 다중 클래스 분류 평가 (각 클래스별 성능)\n",
    "def evaluate_multiclass(y_true, y_pred, class_names=None):\n",
    "    \"\"\"다중 클래스 분류 상세 평가\"\"\"\n",
    "    # 클래스별 정밀도, 재현율, F1-score\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f'Class_{i}' for i in range(len(precision))]\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(\"클래스별 성능:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. 교차 검증\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# K-Fold 교차 검증\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"K-Fold 교차 검증 결과:\")\n",
    "print(f\"평균 정확도: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"각 fold 점수: {cv_scores}\")\n",
    "\n",
    "# 층화 K-Fold (클래스 불균형 데이터)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stratified_scores = cross_val_score(model, X, y, cv=stratified_kfold, scoring='accuracy')\n",
    "\n",
    "print(\"\\n층화 K-Fold 교차 검증 결과:\")\n",
    "print(f\"평균 정확도: {stratified_scores.mean():.4f} (+/- {stratified_scores.std() * 2:.4f})\")\n",
    "\n",
    "# 다중 지표 교차 검증\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "cv_results = cross_validate(model, X, y, cv=stratified_kfold, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print(\"\\n다중 지표 교차 검증 결과:\")\n",
    "for metric in scoring:\n",
    "    test_score = cv_results[f'test_{metric}']\n",
    "    train_score = cv_results[f'train_{metric}']\n",
    "    print(f\"{metric.upper()}:\")\n",
    "    print(f\"  Test:  {test_score.mean():.4f} (+/- {test_score.std() * 2:.4f})\")\n",
    "    print(f\"  Train: {train_score.mean():.4f} (+/- {train_score.std() * 2:.4f})\")\n",
    "\n",
    "# 사용자 정의 평가 지표\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    \"\"\"사용자 정의 평가 함수 예시\"\"\"\n",
    "    # 예: 특정 클래스의 재현율을 중시하는 지표\n",
    "    return recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "custom_metric = make_scorer(custom_scorer)\n",
    "custom_scores = cross_val_score(model, X, y, cv=stratified_kfold, scoring=custom_metric)\n",
    "print(f\"\\n사용자 정의 지표: {custom_scores.mean():.4f} (+/- {custom_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Leave-One-Out 교차 검증 (소규모 데이터셋)\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "if len(X) <= 100:  # 데이터가 적을 때만 사용\n",
    "    loo = LeaveOneOut()\n",
    "    loo_scores = cross_val_score(model, X, y, cv=loo)\n",
    "    print(f\"\\nLeave-One-Out 교차 검증: {loo_scores.mean():.4f}\")\n",
    "\n",
    "# 시계열 데이터용 교차 검증\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ts_scores = cross_val_score(model, X, y, cv=tscv)\n",
    "print(f\"\\n시계열 교차 검증: {ts_scores.mean():.4f} (+/- {ts_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. 하이퍼파라미터 튜닝\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Grid Search CV (격자 탐색)\n",
    "# Random Forest 예시\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "print(\"Random Forest 최적 파라미터:\", rf_grid_search.best_params_)\n",
    "print(\"Random Forest 최적 점수:\", rf_grid_search.best_score_)\n",
    "\n",
    "# Randomized Search CV (랜덤 탐색)\n",
    "# SVM 예시\n",
    "svm = SVC(random_state=42)\n",
    "svm_param_dist = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_random_search = RandomizedSearchCV(\n",
    "    estimator=svm,\n",
    "    param_distributions=svm_param_dist,\n",
    "    n_iter=20,  # 시도할 조합 수\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_random_search.fit(X_train, y_train)\n",
    "print(\"\\nSVM 최적 파라미터:\", svm_random_search.best_params_)\n",
    "print(\"SVM 최적 점수:\", svm_random_search.best_score_)\n",
    "\n",
    "# 최적 모델로 예측 및 평가\n",
    "best_rf = rf_grid_search.best_estimator_\n",
    "best_svm = svm_random_search.best_estimator_\n",
    "\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "svm_pred = best_svm.predict(X_test)\n",
    "\n",
    "print(f\"\\n최적화된 Random Forest 테스트 정확도: {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "print(f\"최적화된 SVM 테스트 정확도: {accuracy_score(y_test, svm_pred):.4f}\")\n",
    "\n",
    "# 파라미터 튜닝 결과 시각화\n",
    "results_df = pd.DataFrame(rf_grid_search.cv_results_)\n",
    "print(\"\\n상위 5개 파라미터 조합:\")\n",
    "print(results_df.nlargest(5, 'mean_test_score')[['mean_test_score', 'params']])\n",
    "\n",
    "# 다중 지표 기반 튜닝\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "multi_scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': f1_scorer,\n",
    "    'precision': 'precision_weighted',\n",
    "    'recall': 'recall_weighted'\n",
    "}\n",
    "\n",
    "multi_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid={'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "    cv=5,\n",
    "    scoring=multi_scoring,\n",
    "    refit='f1',  # f1 점수 기준으로 최적 모델 선택\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "multi_grid_search.fit(X_train, y_train)\n",
    "print(f\"\\n다중 지표 기준 최적 파라미터: {multi_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. 앙상블 방법\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 1. Voting Classifier (투표 앙상블)\n",
    "# Hard Voting\n",
    "base_classifiers = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "hard_voting = VotingClassifier(estimators=base_classifiers, voting='hard')\n",
    "hard_voting.fit(X_train, y_train)\n",
    "hard_pred = hard_voting.predict(X_test)\n",
    "\n",
    "# Soft Voting (확률 기반)\n",
    "soft_voting = VotingClassifier(estimators=base_classifiers, voting='soft')\n",
    "soft_voting.fit(X_train, y_train)\n",
    "soft_pred = soft_voting.predict(X_test)\n",
    "\n",
    "print(f\"Hard Voting 정확도: {accuracy_score(y_test, hard_pred):.4f}\")\n",
    "print(f\"Soft Voting 정확도: {accuracy_score(y_test, soft_pred):.4f}\")\n",
    "\n",
    "# 2. Bagging (Bootstrap Aggregating)\n",
    "bagging = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_pred = bagging.predict(X_test)\n",
    "print(f\"Bagging 정확도: {accuracy_score(y_test, bagging_pred):.4f}\")\n",
    "\n",
    "# 3. AdaBoost (Adaptive Boosting)\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "adaboost.fit(X_train, y_train)\n",
    "ada_pred = adaboost.predict(X_test)\n",
    "print(f\"AdaBoost 정확도: {accuracy_score(y_test, ada_pred):.4f}\")\n",
    "\n",
    "# 4. Gradient Boosting\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "print(f\"Gradient Boosting 정확도: {accuracy_score(y_test, gb_pred):.4f}\")\n",
    "\n",
    "# 5. XGBoost (설치 필요: pip install xgboost)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    print(f\"XGBoost 정확도: {accuracy_score(y_test, xgb_pred):.4f}\")\n",
    "    \n",
    "    # XGBoost 피처 중요도\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\nXGBoost 피처 중요도 (상위 5개):\")\n",
    "    print(xgb_importance.head())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"XGBoost가 설치되지 않았습니다. pip install xgboost로 설치하세요.\")\n",
    "\n",
    "# 6. LightGBM (설치 필요: pip install lightgbm)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42,\n",
    "        verbose=-1  # 로그 출력 억제\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_pred = lgb_model.predict(X_test)\n",
    "    print(f\"LightGBM 정확도: {accuracy_score(y_test, lgb_pred):.4f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"LightGBM이 설치되지 않았습니다. pip install lightgbm으로 설치하세요.\")\n",
    "\n",
    "# 앙상블 모델들의 성능 비교\n",
    "ensemble_results = {\n",
    "    'Hard Voting': accuracy_score(y_test, hard_pred),\n",
    "    'Soft Voting': accuracy_score(y_test, soft_pred),\n",
    "    'Bagging': accuracy_score(y_test, bagging_pred),\n",
    "    'AdaBoost': accuracy_score(y_test, ada_pred),\n",
    "    'Gradient Boosting': accuracy_score(y_test, gb_pred)\n",
    "}\n",
    "\n",
    "print(\"\\n=== 앙상블 모델 성능 비교 ===\")\n",
    "for model, score in sorted(ensemble_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. 신경망 (Neural Network)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 정규화 (신경망은 스케일링이 중요)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 1. 다층 퍼셉트론 분류기\n",
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # 은닉층 구조: 첫 번째 100개, 두 번째 50개 노드\n",
    "    activation='relu',              # 활성화 함수\n",
    "    solver='adam',                  # 최적화 알고리즘\n",
    "    alpha=0.0001,                   # 정규화 파라미터\n",
    "    learning_rate='constant',       # 학습률 스케줄\n",
    "    learning_rate_init=0.001,       # 초기 학습률\n",
    "    max_iter=1000,                  # 최대 반복 횟수\n",
    "    random_state=42,\n",
    "    early_stopping=True,            # 조기 종료\n",
    "    validation_fraction=0.1         # 검증 데이터 비율\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "mlp_pred = mlp_classifier.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_pred)\n",
    "print(f\"MLP Classifier 정확도: {mlp_accuracy:.4f}\")\n",
    "\n",
    "# 학습 과정 시각화 (손실 함수)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mlp_classifier.loss_curve_)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 검증 손실도 함께 표시 (early_stopping=True일 때)\n",
    "if hasattr(mlp_classifier, 'validation_scores_'):\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(mlp_classifier.validation_scores_)\n",
    "    plt.title('Validation Score Curve')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 훈련 반복 횟수: {mlp_classifier.n_iter_}\")\n",
    "print(f\"최종 손실값: {mlp_classifier.loss_:.6f}\")\n",
    "\n",
    "# 2. 다층 퍼셉트론 회귀기 (회귀 문제용)\n",
    "mlp_regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50, 25),  # 3개 은닉층\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    learning_rate='adaptive',           # 적응적 학습률\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "# 회귀 예시 (연속형 타겟이 있을 때)\n",
    "# mlp_regressor.fit(X_train_scaled, y_train_continuous)\n",
    "# mlp_reg_pred = mlp_regressor.predict(X_test_scaled)\n",
    "\n",
    "# 3. 하이퍼파라미터 튜닝\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 50, 25)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Grid Search (시간이 오래 걸릴 수 있음)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp_grid_search = GridSearchCV(\n",
    "    MLPClassifier(max_iter=500, random_state=42),\n",
    "    mlp_param_grid,\n",
    "    cv=3,  # 빠른 실행을 위해 3-fold\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 작은 데이터셋에서만 실행 (시간 고려)\n",
    "if len(X_train) <= 1000:\n",
    "    mlp_grid_search.fit(X_train_scaled, y_train)\n",
    "    print(f\"\\n최적 MLP 파라미터: {mlp_grid_search.best_params_}\")\n",
    "    print(f\"최적 MLP 점수: {mlp_grid_search.best_score_:.4f}\")\n",
    "else:\n",
    "    print(\"\\n데이터가 너무 커서 Grid Search를 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6daffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. 생존 분석 (Survival Analysis)\n",
    "\n",
    "# lifelines 라이브러리 설치 필요: pip install lifelines\n",
    "try:\n",
    "    from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "    from lifelines.statistics import logrank_test\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # 생존 분석용 데이터 예시\n",
    "    # duration: 관찰 기간, event: 사건 발생 여부 (1=사망/실패, 0=중도절단)\n",
    "    # 실제 데이터에서는 df['duration'], df['event'] 컬럼 사용\n",
    "    \n",
    "    # 1. Kaplan-Meier 생존 곡선\n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    # 전체 데이터의 생존 곡선\n",
    "    kmf.fit(durations=df['duration'], event_observed=df['event'], label='Overall')\n",
    "    \n",
    "    # 생존 곡선 시각화\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    kmf.plot_survival_function()\n",
    "    plt.title('Kaplan-Meier Survival Curve')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    \n",
    "    # 그룹별 생존 곡선 비교\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for group in df['group'].unique():\n",
    "        group_data = df[df['group'] == group]\n",
    "        kmf.fit(durations=group_data['duration'], \n",
    "                event_observed=group_data['event'], \n",
    "                label=f'Group {group}')\n",
    "        kmf.plot_survival_function()\n",
    "    \n",
    "    plt.title('Survival Curves by Group')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Log-rank 검정 (그룹간 생존 곡선 차이 검정)\n",
    "    group_0 = df[df['group'] == 0]\n",
    "    group_1 = df[df['group'] == 1]\n",
    "    \n",
    "    results = logrank_test(\n",
    "        group_0['duration'], group_1['duration'],\n",
    "        group_0['event'], group_1['event'],\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    print(\"Log-rank Test 결과:\")\n",
    "    print(f\"Test statistic: {results.test_statistic:.4f}\")\n",
    "    print(f\"p-value: {results.p_value:.4f}\")\n",
    "    print(f\"결론: {'그룹간 생존곡선에 유의한 차이 있음' if results.p_value < 0.05 else '그룹간 생존곡선에 유의한 차이 없음'}\")\n",
    "    \n",
    "    # 3. Cox 비례위험모델\n",
    "    cph = CoxPHFitter()\n",
    "    \n",
    "    # 공변량을 포함한 생존 분석\n",
    "    # 데이터는 duration, event, 그리고 공변량들을 포함해야 함\n",
    "    survival_df = df[['duration', 'event', 'age', 'gender', 'treatment']].copy()\n",
    "    \n",
    "    cph.fit(survival_df, duration_col='duration', event_col='event')\n",
    "    \n",
    "    # Cox 모델 결과 출력\n",
    "    print(\"\\n=== Cox 비례위험모델 결과 ===\")\n",
    "    print(cph.summary)\n",
    "    \n",
    "    # 위험비(Hazard Ratio) 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cph.plot()\n",
    "    plt.title('Hazard Ratios with 95% Confidence Intervals')\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. 생존 확률 예측\n",
    "    # 특정 개체의 생존 함수 예측\n",
    "    individual = pd.DataFrame({\n",
    "        'age': [65],\n",
    "        'gender': [1],\n",
    "        'treatment': [0]\n",
    "    })\n",
    "    \n",
    "    survival_function = cph.predict_survival_function(individual)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    survival_function.plot()\n",
    "    plt.title('Individual Survival Function Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. 중앙 생존 시간\n",
    "    median_survival = kmf.median_survival_time_\n",
    "    print(f\"\\n중앙 생존 시간: {median_survival:.2f}\")\n",
    "    \n",
    "    # 특정 시점에서의 생존 확률\n",
    "    survival_at_t = kmf.survival_function_at_times([12, 24, 36])\n",
    "    print(f\"12개월 생존 확률: {survival_at_t[12]:.4f}\")\n",
    "    print(f\"24개월 생존 확률: {survival_at_t[24]:.4f}\")\n",
    "    print(f\"36개월 생존 확률: {survival_at_t[36]:.4f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"lifelines 라이브러리가 설치되지 않았습니다.\")\n",
    "    print(\"다음 명령어로 설치하세요: pip install lifelines\")\n",
    "    \n",
    "    # lifelines 없이도 사용할 수 있는 기본적인 생존 분석\n",
    "    print(\"\\n기본 생존 분석 (lifelines 없이):\")\n",
    "    \n",
    "    # 생존율 계산 함수\n",
    "    def calculate_survival_rate(duration, event, time_point):\n",
    "        \"\"\"특정 시점에서의 생존율 계산\"\"\"\n",
    "        # 해당 시점까지 관찰된 사람들\n",
    "        at_risk = (duration >= time_point).sum()\n",
    "        # 해당 시점 이전에 사건이 발생한 사람들\n",
    "        events_before = ((duration < time_point) & (event == 1)).sum()\n",
    "        \n",
    "        if at_risk > 0:\n",
    "            survival_rate = (at_risk - events_before) / len(duration)\n",
    "            return survival_rate\n",
    "        return 0\n",
    "    \n",
    "    # 예시 계산\n",
    "    time_points = [6, 12, 18, 24]\n",
    "    for t in time_points:\n",
    "        survival_rate = calculate_survival_rate(df['duration'], df['event'], t)\n",
    "        print(f\"{t}개월 생존율: {survival_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ceffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. 복합 데이터 전처리\n",
    "# 결측치 처리 - 타입별 다른 전략\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 수치형 변수 결측치 처리 (정규성 검정 기반)\n",
    "for col in numeric_cols:\n",
    "    if col != target_col and df[col].isnull().sum() > 0:\n",
    "        # 정규성 검정 후 평균 vs 중앙값 결정\n",
    "        if len(df[col].dropna()) >= 3:\n",
    "            stat, p_value = shapiro(df[col].dropna().sample(min(5000, len(df[col].dropna()))))\n",
    "            if p_value >= 0.05:\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# 범주형 변수 결측치 처리 (최빈값)\n",
    "for col in categorical_cols:\n",
    "    if col != target_col and df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# 이상치 처리 (IQR 방식)\n",
    "for col in numeric_cols:\n",
    "    if col != target_col:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "# 범주형 변수 인코딩 (카디널리티 기반)\n",
    "for col in categorical_cols:\n",
    "    if col != target_col:\n",
    "        if df[col].nunique() <= 10:  # 카디널리티가 낮으면 원핫인코딩\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        else:  # 카디널리티가 높으면 타겟 인코딩\n",
    "            target_mean = df.groupby(col)[target_col].mean()\n",
    "            df[f'{col}_encoded'] = df[col].map(target_mean)\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# 모델 성능 종합 비교\n",
    "models_dict = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42) if 'xgb' in globals() else None\n",
    "}\n",
    "\n",
    "# None 제거\n",
    "models_dict = {k: v for k, v in models_dict.items() if v is not None}\n",
    "\n",
    "# 각 모델 성능 평가\n",
    "results = []\n",
    "for name, model in models_dict.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    result_dict = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "    \n",
    "    # ROC-AUC (이진 분류인 경우)\n",
    "    if len(np.unique(y_test)) == 2 and y_prob is not None:\n",
    "        auc_score = roc_auc_score(y_test, y_prob)\n",
    "        result_dict['ROC-AUC'] = auc_score\n",
    "    \n",
    "    results.append(result_dict)\n",
    "    print(f\"{name} 모델 평가 완료\")\n",
    "\n",
    "# 결과 정리\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "print(\"=== 모델 성능 비교 ===\")\n",
    "print(results_df)\n",
    "\n",
    "# 피처 선택 - 다양한 방법 종합\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_scores = pd.DataFrame(index=X.columns)\n",
    "\n",
    "# F-통계량 기반 점수\n",
    "f_selector = SelectKBest(score_func=f_classif, k='all')\n",
    "f_selector.fit(X, y)\n",
    "feature_scores['f_score'] = f_selector.scores_\n",
    "\n",
    "# 상호 정보량 점수\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "feature_scores['mutual_info'] = mi_scores\n",
    "\n",
    "# Random Forest 중요도\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "feature_scores['rf_importance'] = rf.feature_importances_\n",
    "\n",
    "# 각 방법별 순위 계산\n",
    "for col in ['f_score', 'mutual_info', 'rf_importance']:\n",
    "    feature_scores[f'{col}_rank'] = feature_scores[col].rank(ascending=False)\n",
    "\n",
    "# 종합 순위 (평균 순위)\n",
    "rank_cols = [col for col in feature_scores.columns if '_rank' in col]\n",
    "feature_scores['avg_rank'] = feature_scores[rank_cols].mean(axis=1)\n",
    "feature_scores['final_score'] = feature_scores[['f_score', 'mutual_info', 'rf_importance']].mean(axis=1)\n",
    "\n",
    "# 상위 피처 출력\n",
    "top_k = 10\n",
    "top_features = feature_scores.sort_values('avg_rank').head(top_k)\n",
    "print(f\"\\n상위 {top_k}개 중요 피처:\")\n",
    "print(top_features[['final_score', 'avg_rank']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. 선형회귀와 잔차분석\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy.stats import jarque_bera, shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. 다중 선형 회귀 모델 구축\n",
    "# 상수항 추가 (statsmodels 요구사항)\n",
    "X_with_const = sm.add_constant(X)\n",
    "\n",
    "# 모델 적합\n",
    "model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "print(\"=== 다중 선형 회귀 모델 결과 ===\")\n",
    "print(model.summary())\n",
    "\n",
    "# 2. 회귀 가정 검정\n",
    "predictions = model.fittedvalues\n",
    "residuals = model.resid\n",
    "\n",
    "# 회귀 가정 시각화\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 선형성 가정 (잔차 vs 적합값)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(predictions, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted (선형성 검정)')\n",
    "\n",
    "# 정규성 가정 (Q-Q 플롯)\n",
    "from scipy import stats\n",
    "plt.subplot(2, 3, 2)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (정규성 검정)')\n",
    "\n",
    "# 잔차의 히스토그램\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('잔차 분포')\n",
    "\n",
    "# Scale-Location 플롯 (등분산성)\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(predictions, np.sqrt(np.abs(residuals)), alpha=0.6)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('√|Residuals|')\n",
    "plt.title('Scale-Location Plot (등분산성)')\n",
    "\n",
    "# 잔차의 자기상관\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Observation Order')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('잔차의 순서 플롯')\n",
    "\n",
    "# 영향점 분석 (Cook's Distance)\n",
    "plt.subplot(2, 3, 6)\n",
    "influence = model.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "plt.scatter(range(len(cooks_d)), cooks_d, alpha=0.6)\n",
    "plt.axhline(y=4/len(X), color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel(\"Cook's Distance\")\n",
    "plt.title('영향점 분석')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 통계적 검정 수행\n",
    "print(\"\\n=== 회귀 가정 통계 검정 결과 ===\")\n",
    "\n",
    "# 정규성 검정\n",
    "jb_stat, jb_p = jarque_bera(residuals)\n",
    "sw_stat, sw_p = shapiro(residuals[:5000] if len(residuals) > 5000 else residuals)\n",
    "\n",
    "print(f\"정규성 검정:\")\n",
    "print(f\"  Jarque-Bera test p-value: {jb_p:.6f}\")\n",
    "print(f\"  Shapiro-Wilk test p-value: {sw_p:.6f}\")\n",
    "print(f\"  결론: {'정규성 만족' if min(jb_p, sw_p) >= 0.05 else '정규성 위배'}\")\n",
    "\n",
    "# 등분산성 검정 (Breusch-Pagan test)\n",
    "bp_stat, bp_p, _, _ = het_breuschpagan(residuals, X_with_const)\n",
    "print(f\"\\n등분산성 검정:\")\n",
    "print(f\"  Breusch-Pagan test p-value: {bp_p:.6f}\")\n",
    "print(f\"  결론: {'등분산성 만족' if bp_p >= 0.05 else '이분산성 존재'}\")\n",
    "\n",
    "# 자기상관 검정 (Durbin-Watson test)\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"\\n자기상관 검정:\")\n",
    "print(f\"  Durbin-Watson statistic: {dw_stat:.4f}\")\n",
    "print(f\"  결론: {'자기상관 없음' if 1.5 <= dw_stat <= 2.5 else '자기상관 의심'}\")\n",
    "\n",
    "# 3. 회귀 모델 개선 - VIF 분석\n",
    "print(f\"\\nVIF (분산팽창계수) 분석:\")\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "print(vif_data)\n",
    "\n",
    "high_vif_features = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()\n",
    "if high_vif_features:\n",
    "    print(f\"다중공선성 의심 변수 (VIF > 10): {high_vif_features}\")\n",
    "\n",
    "# 변수 변환 제안 (치우침 기반)\n",
    "print(\"\\n변수 변환 제안:\")\n",
    "for col in X.select_dtypes(include=[np.number]).columns:\n",
    "    skewness = X[col].skew()\n",
    "    if abs(skewness) > 1:\n",
    "        if X[col].min() > 0:  # 모든 값이 양수\n",
    "            if skewness > 1:\n",
    "                print(f\"{col}: 우치우침 → 로그변환 추천\")\n",
    "            elif skewness < -1:\n",
    "                print(f\"{col}: 좌치우침 → 제곱변환 추천\")\n",
    "        else:\n",
    "            print(f\"{col}: 음수 포함 → 절댓값 제곱근 변환 추천\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d237de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. 로지스틱 회귀와 분류 심화\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. 로지스틱 회귀 모델 구축 및 해석\n",
    "# statsmodels을 사용한 상세 분석\n",
    "X_with_const = sm.add_constant(X)\n",
    "logit_model = sm.Logit(y, X_with_const).fit()\n",
    "\n",
    "print(\"=== 로지스틱 회귀 모델 결과 ===\")\n",
    "print(logit_model.summary())\n",
    "\n",
    "# 오즈비(Odds Ratio) 계산\n",
    "odds_ratios = np.exp(logit_model.params)\n",
    "conf_int = np.exp(logit_model.conf_int())\n",
    "\n",
    "odds_summary = pd.DataFrame({\n",
    "    'Coefficient': logit_model.params,\n",
    "    'Odds_Ratio': odds_ratios,\n",
    "    'CI_Lower': conf_int[0],\n",
    "    'CI_Upper': conf_int[1],\n",
    "    'P_Value': logit_model.pvalues\n",
    "})\n",
    "\n",
    "print(\"\\n=== 오즈비 분석 ===\")\n",
    "print(odds_summary.round(4))\n",
    "\n",
    "# 2. 분류 성능 종합 평가\n",
    "# 로지스틱 회귀 모델 학습\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = lr_model.predict(X_test)\n",
    "y_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 기본 성능 지표\n",
    "print(\"=== 기본 성능 지표 ===\")\n",
    "print(f\"정확도: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"정밀도: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"재현율: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-점수: {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# 상세 분류 보고서\n",
    "print(\"\\n=== 상세 분류 보고서 ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 혼동 행렬\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=axes[0,0], cmap='Blues')\n",
    "axes[0,0].set_title('Confusion Matrix')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "axes[0,1].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].set_title('ROC Curve')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "axes[0,2].plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "axes[0,2].set_xlabel('Recall')\n",
    "axes[0,2].set_ylabel('Precision')\n",
    "axes[0,2].set_title('Precision-Recall Curve')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# 확률 분포 비교\n",
    "axes[1,0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='Class 0', density=True)\n",
    "axes[1,0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Class 1', density=True)\n",
    "axes[1,0].set_xlabel('Predicted Probability')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].set_title('Probability Distribution by Class')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 임계값별 성능 변화\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred_thresh))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_thresh))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "axes[1,1].plot(thresholds, precision_scores, label='Precision', marker='o')\n",
    "axes[1,1].plot(thresholds, recall_scores, label='Recall', marker='s')\n",
    "axes[1,1].plot(thresholds, f1_scores, label='F1-Score', marker='^')\n",
    "axes[1,1].set_xlabel('Threshold')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].set_title('Performance vs Threshold')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# 피처 중요도 (로지스틱 회귀의 계수)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=True)\n",
    "\n",
    "axes[1,2].barh(range(len(feature_importance)), feature_importance['coefficient'])\n",
    "axes[1,2].set_yticks(range(len(feature_importance)))\n",
    "axes[1,2].set_yticklabels(feature_importance['feature'])\n",
    "axes[1,2].set_xlabel('Coefficient Value')\n",
    "axes[1,2].set_title('Feature Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. 다중 클래스 분류 분석 (클래스가 3개 이상인 경우)\n",
    "if len(np.unique(y)) > 2:\n",
    "    # 다중 클래스 모델 학습\n",
    "    mc_model = LogisticRegression(random_state=42, multi_class='multinomial')\n",
    "    mc_model.fit(X_train, y_train)\n",
    "    mc_pred = mc_model.predict(X_test)\n",
    "    \n",
    "    # 클래스별 성능\n",
    "    print(\"\\n=== 다중 클래스 분류 성능 ===\")\n",
    "    print(f\"전체 정확도: {accuracy_score(y_test, mc_pred):.4f}\")\n",
    "    \n",
    "    # 클래스별 상세 성능\n",
    "    class_names = [f'Class_{i}' for i in range(len(np.unique(y)))]\n",
    "    report = classification_report(y_test, mc_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    class_performance = pd.DataFrame({\n",
    "        class_name: {\n",
    "            'precision': report[class_name]['precision'],\n",
    "            'recall': report[class_name]['recall'],\n",
    "            'f1-score': report[class_name]['f1-score'],\n",
    "            'support': report[class_name]['support']\n",
    "        } for class_name in class_names\n",
    "    }).T\n",
    "    \n",
    "    print(\"\\n=== 클래스별 상세 성능 ===\")\n",
    "    print(class_performance.round(4))\n",
    "    \n",
    "    # 다중 클래스 혼동 행렬\n",
    "    mc_cm = confusion_matrix(y_test, mc_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(mc_cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "    plt.title('Multi-class Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. 시계열 분석 기초\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# 1. 시계열 데이터 기본 분석\n",
    "# 날짜 컬럼을 인덱스로 설정 (예시)\n",
    "# ts_data = df.set_index('date')['value']  # 실제 사용 시 이렇게 설정\n",
    "\n",
    "# 시계열 데이터 기본 정보\n",
    "print(\"=== 시계열 데이터 기본 정보 ===\")\n",
    "print(f\"데이터 기간: {ts_data.index.min()} ~ {ts_data.index.max()}\")\n",
    "print(f\"관측치 수: {len(ts_data)}\")\n",
    "print(f\"결측치: {ts_data.isnull().sum()}개\")\n",
    "print(f\"평균: {ts_data.mean():.4f}\")\n",
    "print(f\"표준편차: {ts_data.std():.4f}\")\n",
    "\n",
    "# 시계열 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 원본 시계열\n",
    "axes[0,0].plot(ts_data.index, ts_data.values)\n",
    "axes[0,0].set_title('Original Time Series')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Value')\n",
    "\n",
    "# 히스토그램\n",
    "axes[0,1].hist(ts_data.values, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0,1].set_title('Distribution')\n",
    "axes[0,1].set_xlabel('Value')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 1차 차분\n",
    "ts_diff = ts_data.diff().dropna()\n",
    "axes[1,0].plot(ts_diff.index, ts_diff.values)\n",
    "axes[1,0].set_title('First Difference')\n",
    "axes[1,0].set_xlabel('Date')\n",
    "axes[1,0].set_ylabel('Differenced Value')\n",
    "\n",
    "# 계절성 확인 (월별 박스플롯)\n",
    "if len(ts_data) > 12:\n",
    "    monthly_data = ts_data.groupby(ts_data.index.month).apply(list)\n",
    "    monthly_values = [monthly_data[i] for i in range(1, 13) if i in monthly_data.index]\n",
    "    axes[1,1].boxplot(monthly_values)\n",
    "    axes[1,1].set_title('Monthly Distribution')\n",
    "    axes[1,1].set_xlabel('Month')\n",
    "    axes[1,1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. 정상성 검정 (Augmented Dickey-Fuller Test)\n",
    "print(\"\\n=== 정상성 검정 ===\")\n",
    "result = adfuller(ts_data.dropna())\n",
    "\n",
    "print(f\"ADF Statistic: {result[0]:.6f}\")\n",
    "print(f\"p-value: {result[1]:.6f}\")\n",
    "print(f\"Critical Values:\")\n",
    "for key, value in result[4].items():\n",
    "    print(f\"\\t{key}: {value:.6f}\")\n",
    "\n",
    "if result[1] <= 0.05:\n",
    "    print(\"결론: 정상시계열 (p-value ≤ 0.05)\")\n",
    "    d_order = 0\n",
    "else:\n",
    "    print(\"결론: 비정상시계열 (p-value > 0.05)\")\n",
    "    print(\"→ 차분이 필요합니다.\")\n",
    "    \n",
    "    # 1차 차분 후 검정\n",
    "    diff_data = ts_data.diff().dropna()\n",
    "    print(f\"\\n1차 차분 후 ADF 검정:\")\n",
    "    diff_result = adfuller(diff_data)\n",
    "    print(f\"ADF Statistic: {diff_result[0]:.6f}\")\n",
    "    print(f\"p-value: {diff_result[1]:.6f}\")\n",
    "    \n",
    "    if diff_result[1] <= 0.05:\n",
    "        print(\"1차 차분으로 정상성 확보\")\n",
    "        d_order = 1\n",
    "    else:\n",
    "        # 2차 차분\n",
    "        diff2_data = diff_data.diff().dropna()\n",
    "        print(f\"\\n2차 차분 후 ADF 검정:\")\n",
    "        diff2_result = adfuller(diff2_data)\n",
    "        print(f\"ADF Statistic: {diff2_result[0]:.6f}\")\n",
    "        print(f\"p-value: {diff2_result[1]:.6f}\")\n",
    "        \n",
    "        if diff2_result[1] <= 0.05:\n",
    "            print(\"2차 차분으로 정상성 확보\")\n",
    "            d_order = 2\n",
    "        else:\n",
    "            print(\"추가적인 변환이 필요합니다.\")\n",
    "            d_order = 2\n",
    "\n",
    "# 3. 시계열 분해 (Decomposition)\n",
    "# 주기 설정 (데이터 특성에 따라 조정)\n",
    "if ts_data.index.freq:\n",
    "    if 'D' in str(ts_data.index.freq):\n",
    "        period = 365  # 일별 데이터의 연간 주기\n",
    "    elif 'M' in str(ts_data.index.freq):\n",
    "        period = 12   # 월별 데이터의 연간 주기\n",
    "    else:\n",
    "        period = 12\n",
    "else:\n",
    "    period = 12  # 기본값\n",
    "\n",
    "# 분해 수행\n",
    "decomposition = seasonal_decompose(ts_data, model='additive', period=period)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 각 성분의 기여도 분석\n",
    "trend_contribution = (decomposition.trend.var() / ts_data.var()) * 100\n",
    "seasonal_contribution = (decomposition.seasonal.var() / ts_data.var()) * 100\n",
    "residual_contribution = (decomposition.resid.var() / ts_data.var()) * 100\n",
    "\n",
    "print(\"=== 시계열 분해 결과 ===\")\n",
    "print(f\"추세 성분 기여도: {trend_contribution:.2f}%\")\n",
    "print(f\"계절 성분 기여도: {seasonal_contribution:.2f}%\")\n",
    "print(f\"잔차 성분 기여도: {residual_contribution:.2f}%\")\n",
    "\n",
    "# 4. ACF/PACF 분석 및 ARIMA 모델 차수 식별\n",
    "# 정상성을 위한 차분된 데이터 준비\n",
    "if d_order == 1:\n",
    "    stationary_data = ts_data.diff().dropna()\n",
    "elif d_order == 2:\n",
    "    stationary_data = ts_data.diff().diff().dropna()\n",
    "else:\n",
    "    stationary_data = ts_data\n",
    "\n",
    "max_lags = 20\n",
    "\n",
    "# ACF/PACF 플롯\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ACF\n",
    "plot_acf(stationary_data, lags=max_lags, ax=axes[0,0], title='ACF')\n",
    "\n",
    "# PACF\n",
    "plot_pacf(stationary_data, lags=max_lags, ax=axes[0,1], title='PACF')\n",
    "\n",
    "# ACF/PACF 값 계산\n",
    "acf_values = acf(stationary_data, nlags=max_lags)\n",
    "pacf_values = pacf(stationary_data, nlags=max_lags)\n",
    "\n",
    "# ACF 절단점 찾기 (MA 차수 q 추정)\n",
    "axes[1,0].plot(range(len(acf_values)), acf_values, 'bo-')\n",
    "axes[1,0].axhline(y=0, color='black', linestyle='-')\n",
    "axes[1,0].axhline(y=1.96/np.sqrt(len(stationary_data)), color='red', linestyle='--')\n",
    "axes[1,0].axhline(y=-1.96/np.sqrt(len(stationary_data)), color='red', linestyle='--')\n",
    "axes[1,0].set_title('ACF Values')\n",
    "axes[1,0].set_xlabel('Lag')\n",
    "axes[1,0].set_ylabel('ACF')\n",
    "\n",
    "# PACF 절단점 찾기 (AR 차수 p 추정)\n",
    "axes[1,1].plot(range(len(pacf_values)), pacf_values, 'ro-')\n",
    "axes[1,1].axhline(y=0, color='black', linestyle='-')\n",
    "axes[1,1].axhline(y=1.96/np.sqrt(len(stationary_data)), color='red', linestyle='--')\n",
    "axes[1,1].axhline(y=-1.96/np.sqrt(len(stationary_data)), color='red', linestyle='--')\n",
    "axes[1,1].set_title('PACF Values')\n",
    "axes[1,1].set_xlabel('Lag')\n",
    "axes[1,1].set_ylabel('PACF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 자동 차수 추정\n",
    "confidence_interval = 1.96 / np.sqrt(len(stationary_data))\n",
    "\n",
    "# MA 차수 (q) - ACF가 신뢰구간을 벗어나는 마지막 지점\n",
    "q_order = 0\n",
    "for i in range(1, len(acf_values)):\n",
    "    if abs(acf_values[i]) > confidence_interval:\n",
    "        q_order = i\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# AR 차수 (p) - PACF가 신뢰구간을 벗어나는 마지막 지점\n",
    "p_order = 0\n",
    "for i in range(1, len(pacf_values)):\n",
    "    if abs(pacf_values[i]) > confidence_interval:\n",
    "        p_order = i\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"=== ARIMA 모델 차수 추정 ===\")\n",
    "print(f\"추천 ARIMA({p_order}, {d_order}, {q_order}) 모델\")\n",
    "\n",
    "# 5. ARIMA 모델 적합 및 예측\n",
    "forecast_steps = 10\n",
    "order = (p_order, d_order, q_order)\n",
    "\n",
    "print(f\"ARIMA{order} 모델 적합 중...\")\n",
    "\n",
    "# 모델 적합\n",
    "model = ARIMA(ts_data, order=order)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# 모델 요약\n",
    "print(\"=== ARIMA 모델 결과 ===\")\n",
    "print(fitted_model.summary())\n",
    "\n",
    "# 예측\n",
    "forecast = fitted_model.forecast(steps=forecast_steps)\n",
    "forecast_ci = fitted_model.get_forecast(steps=forecast_steps).conf_int()\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 원본 데이터\n",
    "plt.plot(ts_data.index, ts_data.values, label='Observed', color='blue')\n",
    "\n",
    "# 적합값\n",
    "fitted_values = fitted_model.fittedvalues\n",
    "plt.plot(fitted_values.index, fitted_values.values, label='Fitted', color='red', alpha=0.7)\n",
    "\n",
    "# 예측값\n",
    "forecast_index = pd.date_range(start=ts_data.index[-1], periods=forecast_steps+1, freq=ts_data.index.freq)[1:]\n",
    "plt.plot(forecast_index, forecast, label='Forecast', color='green', marker='o')\n",
    "\n",
    "# 신뢰구간\n",
    "plt.fill_between(forecast_index, \n",
    "                 forecast_ci.iloc[:, 0], \n",
    "                 forecast_ci.iloc[:, 1], \n",
    "                 color='green', alpha=0.2)\n",
    "\n",
    "plt.title(f'ARIMA{order} Model Fit and Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 잔차 분석\n",
    "residuals = fitted_model.resid\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 잔차 시계열\n",
    "axes[0,0].plot(residuals.index, residuals.values)\n",
    "axes[0,0].set_title('Residuals')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Residuals')\n",
    "\n",
    "# 잔차 히스토그램\n",
    "axes[0,1].hist(residuals.values, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0,1].set_title('Residuals Distribution')\n",
    "axes[0,1].set_xlabel('Residuals')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 잔차 ACF\n",
    "plot_acf(residuals.dropna(), lags=20, ax=axes[1,0], title='Residuals ACF')\n",
    "\n",
    "# Q-Q 플롯\n",
    "from scipy import stats\n",
    "stats.probplot(residuals.dropna(), dist=\"norm\", plot=axes[1,1])\n",
    "axes[1,1].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
